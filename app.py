import os
from flask import Flask, render_template, request, jsonify, redirect, url_for
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import text
from datetime import datetime, timezone
import feedparser
import requests
from bs4 import BeautifulSoup
from apscheduler.schedulers.background import BackgroundScheduler
import logging
import hashlib
from urllib.parse import urljoin, urlparse
import re
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///rss_feeds.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)

# æ•°æ®åº“æ¨¡å‹
class RSSSource(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    url = db.Column(db.String(500), nullable=False, unique=True)
    category = db.Column(db.String(50), default='general')
    active = db.Column(db.Boolean, default=True)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

class Article(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(500), nullable=False)
    url = db.Column(db.String(1000), nullable=False, unique=True)
    description = db.Column(db.Text)
    content = db.Column(db.Text)
    summary = db.Column(db.Text)  # æ–°å¢ï¼šAIç”Ÿæˆçš„æ–‡ç« æ‘˜è¦
    author = db.Column(db.String(100))
    published_date = db.Column(db.DateTime)
    source_id = db.Column(db.Integer, db.ForeignKey('rss_source.id'), nullable=False)
    tags = db.Column(db.String(500))  # é€—å·åˆ†éš”çš„æ ‡ç­¾
    read_status = db.Column(db.Boolean, default=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    source = db.relationship('RSSSource', backref=db.backref('articles', lazy=True))

# å†…å®¹æå–å’Œæ‘˜è¦ç”Ÿæˆ
class ContentProcessor:
    @staticmethod
    def extract_article_content(url):
        """ä»æ–‡ç« URLæå–æ­£æ–‡å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
            
            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            content_selectors = [
                'article', '.article', '#article',
                '.content', '#content', '.post-content',
                '.entry-content', '.post-body', '.article-body',
                'main', '.main', '#main'
            ]
            
            content = None
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0]
                    break
            
            if not content:
                # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
                content = soup.find('body')
            
            if content:
                # æå–æ–‡æœ¬å¹¶æ¸…ç†
                text = content.get_text()
                # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                text = re.sub(r'\s+', ' ', text).strip()
                return text[:5000]  # é™åˆ¶é•¿åº¦
            
            return ""
            
        except Exception as e:
            logger.warning(f"æ— æ³•æå–æ–‡ç« å†…å®¹ {url}: {str(e)}")
            return ""
    
    @staticmethod
    def generate_summary(title, description, content):
        """ç”Ÿæˆç»“æ„åŒ–çš„æŠ€æœ¯æ‘˜è¦"""
        try:
            # åˆå¹¶æ‰€æœ‰å¯ç”¨çš„æ–‡æœ¬å†…å®¹
            full_text = ""
            if title:
                full_text += f"æ ‡é¢˜: {title}\n"
            if description:
                full_text += f"æè¿°: {description}\n"
            if content:
                full_text += f"å†…å®¹: {content[:3000]}\n"  # å¢åŠ å†…å®¹é•¿åº¦é™åˆ¶
            
            if not full_text.strip():
                return "æš‚æ— å†…å®¹æ‘˜è¦"
            
            # ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦
            summary = ContentProcessor._generate_structured_summary(full_text, title)
            return summary
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡"
    
    @staticmethod
    def _generate_structured_summary(text, title=""):
        """ç”Ÿæˆç»“æ„åŒ–çš„æŠ€æœ¯æ‘˜è¦ï¼ŒåŒ…å«å…³é”®æŠ€æœ¯ç‚¹å’Œè®ºç‚¹"""
        
        # æŠ€æœ¯é¢†åŸŸåˆ†ç±»å’Œå…³é”®è¯
        tech_categories = {
            'æ¸¸æˆå¼•æ“': ['unreal', 'unity', 'godot', 'engine', 'framework', 'runtime'],
            'æ¸²æŸ“æŠ€æœ¯': ['render', 'shader', 'graphics', 'gpu', 'vulkan', 'directx', 'opengl', 'lighting', 'shadow', 'material'],
            'ç‰©ç†ä»¿çœŸ': ['physics', 'collision', 'rigidbody', 'simulation', 'dynamics', 'constraint'],
            'åŠ¨ç”»ç³»ç»Ÿ': ['animation', 'skeletal', 'blend', 'timeline', 'motion', 'ik', 'bone'],
            'äººå·¥æ™ºèƒ½': ['ai', 'ml', 'neural', 'behavior', 'pathfinding', 'decision', 'learning'],
            'æ€§èƒ½ä¼˜åŒ–': ['optimization', 'performance', 'profiling', 'memory', 'cpu', 'fps', 'bottleneck'],
            'æ¶æ„è®¾è®¡': ['architecture', 'pattern', 'design', 'component', 'system', 'modular', 'ecs'],
            'ç½‘ç»œç¼–ç¨‹': ['network', 'multiplayer', 'server', 'client', 'synchronization', 'latency'],
            'è™šæ‹Ÿç°å®': ['vr', 'ar', 'xr', 'virtual', 'augmented', 'headset', 'tracking'],
            'å·¥å…·å¼€å‘': ['tool', 'editor', 'pipeline', 'automation', 'workflow', 'asset']
        }
        
        # æŠ€æœ¯æ–¹æ³•å’Œå®ç°å…³é”®è¯
        implementation_keywords = [
            'implement', 'algorithm', 'approach', 'method', 'technique', 'solution',
            'å®ç°', 'ç®—æ³•', 'æ–¹æ³•', 'æŠ€æœ¯', 'è§£å†³æ–¹æ¡ˆ', 'ç­–ç•¥'
        ]
        
        # é—®é¢˜å’ŒæŒ‘æˆ˜å…³é”®è¯
        problem_keywords = [
            'problem', 'issue', 'challenge', 'limitation', 'bottleneck', 'bug',
            'é—®é¢˜', 'æŒ‘æˆ˜', 'é™åˆ¶', 'ç“¶é¢ˆ', 'å›°éš¾', 'ç¼ºé™·'
        ]
        
        # ç»“æœå’Œæ•ˆæœå…³é”®è¯
        result_keywords = [
            'result', 'performance', 'improvement', 'benefit', 'advantage', 'effect',
            'ç»“æœ', 'æ€§èƒ½', 'æ”¹è¿›', 'ä¼˜åŠ¿', 'æ•ˆæœ', 'æå‡'
        ]
        
        # æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 15]
        
        # åˆ†ææ¯ä¸ªå¥å­
        analyzed_sentences = []
        for sentence in sentences[:20]:  # åˆ†æå‰20ä¸ªå¥å­
            analysis = ContentProcessor._analyze_sentence(sentence, tech_categories, 
                                                        implementation_keywords, 
                                                        problem_keywords, 
                                                        result_keywords)
            if analysis['relevance_score'] > 0:
                analyzed_sentences.append(analysis)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        analyzed_sentences.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # æ„å»ºç»“æ„åŒ–æ‘˜è¦
        summary_parts = []
        
        # 1. æŠ€æœ¯é¢†åŸŸæ¦‚è¿°
        tech_areas = ContentProcessor._extract_tech_areas(analyzed_sentences)
        if tech_areas:
            summary_parts.append(f"ğŸ“‹ **æŠ€æœ¯é¢†åŸŸ**: {', '.join(tech_areas)}")
        
        # 2. å…³é”®æŠ€æœ¯ç‚¹
        key_points = ContentProcessor._extract_key_points_structured(analyzed_sentences, 'implementation')
        if key_points:
            summary_parts.append("ğŸ”§ **å…³é”®æŠ€æœ¯ç‚¹**:")
            for i, point in enumerate(key_points[:3], 1):
                summary_parts.append(f"   {i}. {point}")
        
        # 3. ä¸»è¦è®ºç‚¹/è§‚ç‚¹
        main_arguments = ContentProcessor._extract_key_points_structured(analyzed_sentences, 'argument')
        if main_arguments:
            summary_parts.append("ğŸ’¡ **ä¸»è¦è®ºç‚¹**:")
            for i, arg in enumerate(main_arguments[:3], 1):
                summary_parts.append(f"   {i}. {arg}")
        
        # 4. é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
        problems_solutions = ContentProcessor._extract_problems_solutions(analyzed_sentences)
        if problems_solutions:
            summary_parts.append("âš¡ **é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ**:")
            for i, ps in enumerate(problems_solutions[:2], 1):
                summary_parts.append(f"   {i}. {ps}")
        
        # 5. æ€§èƒ½/æ•ˆæœ
        results = ContentProcessor._extract_key_points_structured(analyzed_sentences, 'result')
        if results:
            summary_parts.append("ğŸ“ˆ **æ•ˆæœä¸æ”¶ç›Š**:")
            for i, result in enumerate(results[:2], 1):
                summary_parts.append(f"   {i}. {result}")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œç”ŸæˆåŸºç¡€æ‘˜è¦
        if len(summary_parts) < 2:
            summary_parts = [f"è¿™æ˜¯ä¸€ç¯‡å…³äº{tech_areas[0] if tech_areas else 'æ¸¸æˆå¼€å‘'}çš„æŠ€æœ¯æ–‡ç« "]
            if analyzed_sentences:
                best_sentence = ContentProcessor._simplify_to_chinese(analyzed_sentences[0]['text'])
                if best_sentence:
                    summary_parts.append(best_sentence)
        
        # ç»„åˆæ‘˜è¦
        summary = '\n'.join(summary_parts)
        
        # é™åˆ¶æ€»é•¿åº¦
        if len(summary) > 500:
            lines = summary.split('\n')
            summary = '\n'.join(lines[:8]) + '\n...'
        
        return summary
    
    @staticmethod
    def _analyze_sentence(sentence, tech_categories, impl_keywords, prob_keywords, result_keywords):
        """åˆ†æå¥å­çš„æŠ€æœ¯ç›¸å…³æ€§å’Œç±»å‹"""
        sentence_lower = sentence.lower()
        
        analysis = {
            'text': sentence,
            'tech_areas': [],
            'type': 'general',
            'relevance_score': 0,
            'keywords': []
        }
        
        # æ£€æŸ¥æŠ€æœ¯é¢†åŸŸ
        for area, keywords in tech_categories.items():
            for keyword in keywords:
                if keyword in sentence_lower:
                    analysis['tech_areas'].append(area)
                    analysis['keywords'].append(keyword)
                    analysis['relevance_score'] += 2
        
        # ç¡®å®šå¥å­ç±»å‹
        if any(kw in sentence_lower for kw in impl_keywords):
            analysis['type'] = 'implementation'
            analysis['relevance_score'] += 3
        elif any(kw in sentence_lower for kw in prob_keywords):
            analysis['type'] = 'problem'
            analysis['relevance_score'] += 2
        elif any(kw in sentence_lower for kw in result_keywords):
            analysis['type'] = 'result'
            analysis['relevance_score'] += 2
        elif any(word in sentence_lower for word in ['æ–°', 'new', 'åˆ›æ–°', 'innovative', 'æå‡º', 'propose']):
            analysis['type'] = 'argument'
            analysis['relevance_score'] += 2
        
        # é¢å¤–åŠ åˆ†é¡¹
        if re.search(r'\d+%|\d+å€|\d+x', sentence):  # åŒ…å«æ•°å­—/ç™¾åˆ†æ¯”
            analysis['relevance_score'] += 1
        if len(sentence) > 30 and len(sentence) < 150:  # é•¿åº¦é€‚ä¸­
            analysis['relevance_score'] += 1
        
        return analysis
    
    @staticmethod
    def _extract_tech_areas(analyzed_sentences):
        """æå–ä¸»è¦æŠ€æœ¯é¢†åŸŸ"""
        area_count = {}
        for sentence in analyzed_sentences:
            for area in sentence['tech_areas']:
                area_count[area] = area_count.get(area, 0) + 1
        
        # è¿”å›å‡ºç°é¢‘ç‡æœ€é«˜çš„æŠ€æœ¯é¢†åŸŸ
        sorted_areas = sorted(area_count.items(), key=lambda x: x[1], reverse=True)
        return [area for area, count in sorted_areas[:3]]
    
    @staticmethod
    def _extract_key_points_structured(analyzed_sentences, point_type):
        """æå–ç‰¹å®šç±»å‹çš„å…³é”®ç‚¹"""
        points = []
        for sentence in analyzed_sentences:
            if sentence['type'] == point_type and sentence['relevance_score'] >= 3:
                simplified = ContentProcessor._simplify_to_chinese(sentence['text'])
                if simplified and len(simplified) > 10:
                    points.append(simplified)
        
        return points[:3]  # æœ€å¤šè¿”å›3ä¸ªç‚¹
    
    @staticmethod
    def _extract_problems_solutions(analyzed_sentences):
        """æå–é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆå¯¹"""
        problems = []
        solutions = []
        
        for sentence in analyzed_sentences:
            if sentence['type'] == 'problem':
                problems.append(ContentProcessor._simplify_to_chinese(sentence['text']))
            elif sentence['type'] == 'implementation':
                solutions.append(ContentProcessor._simplify_to_chinese(sentence['text']))
        
        # ç»„åˆé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
        combined = []
        for i in range(min(len(problems), len(solutions))):
            if problems[i] and solutions[i]:
                combined.append(f"é—®é¢˜: {problems[i][:50]}... â†’ è§£å†³: {solutions[i][:50]}...")
        
        return combined[:2]
    
    @staticmethod
    def _simplify_to_chinese(text):
        """å°†è‹±æ–‡æŠ€æœ¯å¥å­ç®€åŒ–å¹¶ä¸­æ–‡åŒ–"""
        if not text:
            return ""
        
        # å¸¸è§æŠ€æœ¯æœ¯è¯­çš„ä¸­æ–‡æ›¿æ¢
        replacements = {
            'Unreal Engine': 'Unrealå¼•æ“',
            'Unity': 'Unityå¼•æ“',
            'rendering': 'æ¸²æŸ“',
            'performance': 'æ€§èƒ½',
            'optimization': 'ä¼˜åŒ–',
            'shader': 'ç€è‰²å™¨',
            'physics': 'ç‰©ç†',
            'animation': 'åŠ¨ç”»',
            'AI': 'äººå·¥æ™ºèƒ½',
            'VR': 'è™šæ‹Ÿç°å®',
            'AR': 'å¢å¼ºç°å®',
            'GPU': 'å›¾å½¢å¤„ç†å™¨',
            'CPU': 'å¤„ç†å™¨',
            'framework': 'æ¡†æ¶',
            'algorithm': 'ç®—æ³•',
            'gameplay': 'æ¸¸æˆç©æ³•'
        }
        
        # åº”ç”¨æ›¿æ¢
        result = text
        for eng, cn in replacements.items():
            result = re.sub(r'\b' + re.escape(eng) + r'\b', cn, result, flags=re.IGNORECASE)
        
        # ç§»é™¤è¿‡é•¿çš„æŠ€æœ¯ç»†èŠ‚
        if len(result) > 100:
            # å°è¯•æå–ä¸»è¦ä¿¡æ¯
            sentences = re.split(r'[.ã€‚]', result)
            if sentences:
                result = sentences[0] + "ã€‚"
        
        return result.strip()

# RSSæŠ“å–åŠŸèƒ½
class RSSFetcher:
    @staticmethod
    def fetch_articles(source):
        try:
            logger.info(f"æ­£åœ¨æŠ“å–RSSæº: {source.name}")
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´
            import socket
            socket.setdefaulttimeout(30)  # 30ç§’è¶…æ—¶
            
            feed = feedparser.parse(source.url)
            
            if feed.bozo:
                logger.warning(f"RSSæºå¯èƒ½æœ‰é—®é¢˜: {source.name} - {feed.bozo_exception}")
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"RSSæºæ²¡æœ‰æ–‡ç« : {source.name}")
                return 0
            
            logger.info(f"RSSæº {source.name} æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
            
            new_articles = 0
            # é™åˆ¶æ¯æ¬¡å¤„ç†çš„æ–‡ç« æ•°é‡ï¼Œé¿å…è¶…æ—¶
            max_articles = 10
            
            for i, entry in enumerate(feed.entries[:max_articles]):
                try:
                    logger.info(f"æ­£åœ¨æ£€æŸ¥æ–‡ç« : {entry.title[:50]}... (URL: {entry.link})")
                    
                    # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
                    existing = Article.query.filter_by(url=entry.link).first()
                    if existing:
                        logger.info(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {entry.title[:50]}...")
                        continue
                    
                    logger.info(f"æ–‡ç« ä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†: {entry.title[:50]}...")
                    
                    # è§£æå‘å¸ƒæ—¥æœŸ
                    published_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_date = datetime(*entry.published_parsed[:6])
                        except (ValueError, TypeError):
                            pass
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        try:
                            published_date = datetime(*entry.updated_parsed[:6])
                        except (ValueError, TypeError):
                            pass
                    
                    # æå–æè¿°
                    description = ""
                    if hasattr(entry, 'summary'):
                        description = entry.summary
                    elif hasattr(entry, 'description'):
                        description = entry.description
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    if description:
                        soup = BeautifulSoup(description, 'html.parser')
                        description = soup.get_text()[:500]
                    
                    # æå–æ ‡ç­¾
                    tags = []
                    if hasattr(entry, 'tags'):
                        tags = [tag.term for tag in entry.tags]
                    
                    # æå–æ–‡ç« å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦ï¼ˆç®€åŒ–å¤„ç†ï¼Œé¿å…è¶…æ—¶ï¼‰
                    logger.info(f"æ­£åœ¨å¤„ç†æ–‡ç«  {i+1}/{min(len(feed.entries), max_articles)}: {entry.title[:50]}...")
                    
                    # ç®€åŒ–å†…å®¹æå–ï¼Œé¿å…è¶…æ—¶
                    article_content = ""
                    article_summary = ContentProcessor.generate_summary(
                        entry.title, 
                        description, 
                        ""  # æš‚æ—¶ä¸æå–å®Œæ•´å†…å®¹ï¼Œé¿å…è¶…æ—¶
                    )
                    
                    # åˆ›å»ºæ–°æ–‡ç« 
                    article = Article(
                        title=entry.title[:500],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        url=entry.link,
                        description=description,
                        content=article_content,
                        summary=article_summary,
                        author=getattr(entry, 'author', '')[:100],  # é™åˆ¶ä½œè€…é•¿åº¦
                        published_date=published_date,
                        source_id=source.id,
                        tags=','.join(tags)[:500]  # é™åˆ¶æ ‡ç­¾é•¿åº¦
                    )
                    
                    db.session.add(article)
                    new_articles += 1
                    logger.info(f"æˆåŠŸæ·»åŠ æ–‡ç« : {entry.title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æ›´æ–°æºçš„æœ€åæ›´æ–°æ—¶é—´
            source.last_updated = datetime.utcnow()
            db.session.commit()
            
            logger.info(f"ä» {source.name} æŠ“å–äº† {new_articles} ç¯‡æ–°æ–‡ç« ")
            return new_articles
            
        except Exception as e:
            logger.error(f"æŠ“å–RSSæº {source.name} æ—¶å‡ºé”™: {str(e)}")
            db.session.rollback()  # å›æ»šäº‹åŠ¡
            return 0
    
    @staticmethod
    def fetch_all_sources():
        """æŠ“å–æ‰€æœ‰RSSæºçš„æ–‡ç« """
        try:
            logger.info("å¼€å§‹æŠ“å–æ‰€æœ‰RSSæº")
            sources = RSSSource.query.filter_by(active=True).all()
            
            logger.info(f"æ‰¾åˆ° {len(sources)} ä¸ªæ´»è·ƒçš„RSSæº")
            
            if not sources:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„RSSæº")
                return 0
            
            total_new_articles = 0
            successful_sources = 0
            failed_sources = 0
            
            for i, source in enumerate(sources):
                try:
                    logger.info(f"æ­£åœ¨å¤„ç†RSSæº {i+1}/{len(sources)}: {source.name} (URL: {source.url})")
                    new_articles = RSSFetcher.fetch_articles(source)
                    total_new_articles += new_articles
                    successful_sources += 1
                    logger.info(f"RSSæº {source.name} å®Œæˆï¼Œæ–°å¢ {new_articles} ç¯‡æ–‡ç« ")
                    
                except Exception as e:
                    failed_sources += 1
                    logger.error(f"å¤„ç†RSSæº {source.name} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            logger.info(f"RSSæŠ“å–å®Œæˆï¼æˆåŠŸ: {successful_sources}, å¤±è´¥: {failed_sources}, æ€»æ–°å¢æ–‡ç« : {total_new_articles}")
            return total_new_articles
            
        except Exception as e:
            logger.error(f"æŠ“å–æ‰€æœ‰RSSæºæ—¶å‡ºé”™: {str(e)}")
            return 0

# è·¯ç”±
@app.route('/')
def index():
    page = request.args.get('page', 1, type=int)
    category = request.args.get('category', '')
    search = request.args.get('search', '')
    
    query = Article.query
    
    if category:
        source_ids = [s.id for s in RSSSource.query.filter_by(category=category).all()]
        query = query.filter(Article.source_id.in_(source_ids))
    
    if search:
        query = query.filter(
            db.or_(
                Article.title.contains(search),
                Article.description.contains(search),
                Article.tags.contains(search)
            )
        )
    
    articles = query.order_by(Article.published_date.desc()).paginate(
        page=page, per_page=20, error_out=False
    )
    
    # è·å–åˆ†ç±»ç»Ÿè®¡
    categories = db.session.query(RSSSource.category, db.func.count(Article.id)).join(Article).group_by(RSSSource.category).all()
    
    return render_template('index.html', articles=articles, categories=categories, current_category=category, search=search)

@app.route('/sources')
def sources():
    sources = RSSSource.query.all()
    return render_template('sources.html', sources=sources)

@app.route('/add_source', methods=['POST'])
def add_source():
    name = request.form.get('name')
    url = request.form.get('url')
    category = request.form.get('category', 'general')
    
    if not name or not url:
        return jsonify({'error': 'åç§°å’ŒURLä¸èƒ½ä¸ºç©º'}), 400
    
    # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
    existing = RSSSource.query.filter_by(url=url).first()
    if existing:
        return jsonify({'error': 'RSSæºå·²å­˜åœ¨'}), 400
    
    source = RSSSource(name=name, url=url, category=category)
    db.session.add(source)
    db.session.commit()
    
    # ç«‹å³æŠ“å–ä¸€æ¬¡
    RSSFetcher.fetch_articles(source)
    
    return redirect(url_for('sources'))

@app.route('/delete_source/<int:source_id>', methods=['POST'])
def delete_source(source_id):
    source = RSSSource.query.get_or_404(source_id)
    
    # åˆ é™¤ç›¸å…³æ–‡ç« 
    Article.query.filter_by(source_id=source_id).delete()
    
    # åˆ é™¤RSSæº
    db.session.delete(source)
    db.session.commit()
    
    return redirect(url_for('sources'))

@app.route('/toggle_source/<int:source_id>', methods=['POST'])
def toggle_source(source_id):
    source = RSSSource.query.get_or_404(source_id)
    source.active = not source.active
    db.session.commit()
    return redirect(url_for('sources'))

@app.route('/fetch_now', methods=['GET', 'POST'])
def fetch_now():
    try:
        print("=" * 50)
        print("ğŸš€ æ‰‹åŠ¨æŠ“å–RSSæºè¯·æ±‚å·²æ¥æ”¶ï¼")
        print("=" * 50)
        logger.info("ğŸš€ å¼€å§‹æ‰‹åŠ¨æŠ“å–RSSæº")
        
        # è®¾ç½®æ›´ä¸¥æ ¼çš„è¶…æ—¶æ§åˆ¶
        import signal
        import threading
        
        def timeout_handler():
            logger.error("RSSæŠ“å–è¶…æ—¶")
            return None
        
        # ä½¿ç”¨çº¿ç¨‹æ¥æ‰§è¡ŒæŠ“å–ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        result = {'new_articles': 0, 'error': None}
        
        def fetch_worker():
            try:
                print("ğŸ“¡ å¼€å§‹æ‰§è¡ŒæŠ“å–å·¥ä½œçº¿ç¨‹")
                # åœ¨çº¿ç¨‹ä¸­åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                with app.app_context():
                    result['new_articles'] = RSSFetcher.fetch_all_sources()
                print(f"âœ… æŠ“å–å·¥ä½œçº¿ç¨‹å®Œæˆï¼Œè·å¾— {result['new_articles']} ç¯‡æ–°æ–‡ç« ")
            except Exception as e:
                result['error'] = str(e)
                print(f"âŒ æŠ“å–çº¿ç¨‹å‡ºé”™: {str(e)}")
                logger.error(f"æŠ“å–çº¿ç¨‹å‡ºé”™: {str(e)}")
        
        # åˆ›å»ºå¹¶å¯åŠ¨æŠ“å–çº¿ç¨‹
        fetch_thread = threading.Thread(target=fetch_worker)
        fetch_thread.daemon = True
        fetch_thread.start()
        print("ğŸ”„ æŠ“å–çº¿ç¨‹å·²å¯åŠ¨ï¼Œç­‰å¾…å®Œæˆ...")
        
        # ç­‰å¾…æœ€å¤š60ç§’
        fetch_thread.join(timeout=60)
        
        if fetch_thread.is_alive():
            print("â° RSSæŠ“å–è¶…æ—¶ï¼ˆ60ç§’ï¼‰")
            logger.error("RSSæŠ“å–è¶…æ—¶ï¼ˆ60ç§’ï¼‰")
            return jsonify({
                'success': False, 
                'message': 'æŠ“å–è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•'
            }), 408
        
        if result['error']:
            print(f"âŒ RSSæŠ“å–å‡ºé”™: {result['error']}")
            logger.error(f"RSSæŠ“å–å‡ºé”™: {result['error']}")
            return jsonify({
                'success': False, 
                'message': f'æŠ“å–å¤±è´¥: {result["error"]}'
            }), 500
        
        new_articles = result['new_articles']
        print(f"ğŸ‰ æ‰‹åŠ¨æŠ“å–å®Œæˆï¼Œè·å¾— {new_articles} ç¯‡æ–°æ–‡ç« ")
        logger.info(f"æ‰‹åŠ¨æŠ“å–å®Œæˆï¼Œè·å¾— {new_articles} ç¯‡æ–°æ–‡ç« ")
        return jsonify({
            'success': True, 
            'message': f'æŠ“å–å®Œæˆï¼Œè·å¾— {new_articles} ç¯‡æ–°æ–‡ç« '
        })
        
    except Exception as e:
        print(f"ğŸ’¥ æ‰‹åŠ¨æŠ“å–RSSæºæ—¶å‡ºé”™: {str(e)}")
        logger.error(f"æ‰‹åŠ¨æŠ“å–RSSæºæ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            'success': False, 
            'message': f'æŠ“å–å¤±è´¥: {str(e)}'
        }), 500

@app.route('/article/<int:article_id>')
def article_detail(article_id):
    article = Article.query.get_or_404(article_id)
    
    # æ ‡è®°ä¸ºå·²è¯»
    article.read_status = True
    db.session.commit()
    
    return render_template('article.html', article=article)

@app.route('/mark_read/<int:article_id>', methods=['POST'])
def mark_read(article_id):
    article = Article.query.get_or_404(article_id)
    article.read_status = True
    db.session.commit()
    return jsonify({'success': True})

@app.route('/mark_all_read', methods=['POST'])
def mark_all_read():
    Article.query.update({'read_status': True})
    db.session.commit()
    return jsonify({'success': True, 'message': 'æ‰€æœ‰æ–‡ç« å·²æ ‡è®°ä¸ºå·²è¯»'})

@app.route('/clear_data', methods=['POST'])
def clear_data():
    """æ¸…é™¤æ‰€æœ‰æ–‡ç« æ•°æ®å¹¶é‡ç½®RSSæºçŠ¶æ€"""
    try:
        logger.info("å¼€å§‹æ¸…é™¤æ‰€æœ‰æ–‡ç« æ•°æ®")
        
        # åˆ é™¤æ‰€æœ‰æ–‡ç« 
        deleted_count = Article.query.count()
        Article.query.delete()
        
        # é‡ç½®æ‰€æœ‰RSSæºçš„æœ€åæ›´æ–°æ—¶é—´ï¼Œå¹¶ç¡®ä¿å®ƒä»¬æ˜¯æ´»è·ƒçš„
        sources = RSSSource.query.all()
        for source in sources:
            source.last_updated = None
            source.active = True  # ç¡®ä¿RSSæºæ˜¯æ´»è·ƒçš„
        
        db.session.commit()
        
        logger.info(f"æˆåŠŸæ¸…é™¤ {deleted_count} ç¯‡æ–‡ç« ï¼Œé‡ç½® {len(sources)} ä¸ªRSSæº")
        return jsonify({
            'success': True, 
            'message': f'æˆåŠŸæ¸…é™¤ {deleted_count} ç¯‡æ–‡ç« å’Œæ‰€æœ‰å·²è¯»è®°å½•ï¼Œé‡ç½® {len(sources)} ä¸ªRSSæº'
        })
        
    except Exception as e:
        logger.error(f"æ¸…é™¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        db.session.rollback()
        return jsonify({
            'success': False, 
            'message': f'æ¸…é™¤æ•°æ®å¤±è´¥: {str(e)}'
        }), 500

@app.route('/debug/sources')
def debug_sources():
    """è°ƒè¯•è·¯ç”±ï¼šæŸ¥çœ‹RSSæºçŠ¶æ€"""
    sources = RSSSource.query.all()
    source_info = []
    for source in sources:
        source_info.append({
            'id': source.id,
            'name': source.name,
            'url': source.url,
            'active': source.active,
            'last_updated': source.last_updated.isoformat() if source.last_updated else None,
            'category': source.category
        })
    
    return jsonify({
        'total_sources': len(sources),
        'active_sources': len([s for s in sources if s.active]),
        'sources': source_info
    })

@app.route('/test', methods=['GET', 'POST'])
def test_route():
    """æµ‹è¯•è·¯ç”±ï¼šéªŒè¯æœåŠ¡å™¨å“åº”"""
    print("=" * 50)
    print("ğŸ§ª æµ‹è¯•è·¯ç”±è¢«è°ƒç”¨ï¼")
    print(f"è¯·æ±‚æ–¹æ³•: {request.method}")
    print("=" * 50)
    return jsonify({
        'success': True,
        'message': 'æµ‹è¯•è·¯ç”±æ­£å¸¸å·¥ä½œï¼',
        'method': request.method
    })

# åˆå§‹åŒ–æ•°æ®åº“å’Œé»˜è®¤RSSæº
def init_db():
    with app.app_context():
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ summaryåˆ—
        try:
            db.session.execute(text("SELECT summary FROM article LIMIT 1"))
        except Exception:
            # summaryåˆ—ä¸å­˜åœ¨ï¼Œæ·»åŠ å®ƒ
            logger.info("æ·»åŠ summaryåˆ—åˆ°articleè¡¨")
            db.session.execute(text("ALTER TABLE article ADD COLUMN summary TEXT"))
            db.session.commit()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ contentåˆ—
        try:
            db.session.execute(text("SELECT content FROM article LIMIT 1"))
        except Exception:
            # contentåˆ—ä¸å­˜åœ¨ï¼Œæ·»åŠ å®ƒ
            logger.info("æ·»åŠ contentåˆ—åˆ°articleè¡¨")
            db.session.execute(text("ALTER TABLE article ADD COLUMN content TEXT"))
            db.session.commit()
        
        db.create_all()
        
        # æ·»åŠ ä¸“æ³¨äºæ¸¸æˆå¼€å‘æŠ€æœ¯çš„RSSæº
        default_sources = [
            # å¼•æ“æŠ€æœ¯
            {
                'name': 'Unreal Engine Blog',
                'url': 'https://www.unrealengine.com/en-US/feed',
                'category': 'unreal_engine'
            },
            {
                'name': 'Unity Blog',
                'url': 'https://blog.unity.com/feed',
                'category': 'unity'
            },
            {
                'name': 'Godot Engine News',
                'url': 'https://godotengine.org/rss.xml',
                'category': 'game_engines'
            },
            
            # æ¸¸æˆå¼€å‘ç»¼åˆ
            {
                'name': 'Game Developer (Gamasutra)',
                'url': 'https://www.gamedeveloper.com/rss.xml',
                'category': 'game_development'
            },
            {
                'name': 'Indie Game Developer',
                'url': 'https://www.indiegamedev.net/feed/',
                'category': 'indie_development'
            },
            
            # å›¾å½¢ç¼–ç¨‹å’Œæ¸²æŸ“æŠ€æœ¯
            {
                'name': 'Real-Time Rendering',
                'url': 'http://www.realtimerendering.com/blog/feed/',
                'category': 'graphics_programming'
            },
            {
                'name': 'Graphics Programming Weekly',
                'url': 'https://www.jendrikillner.com/tags/weekly/index.xml',
                'category': 'graphics_programming'
            },
            {
                'name': 'Advances in Real-Time Rendering',
                'url': 'http://advances.realtimerendering.com/feed/',
                'category': 'graphics_programming'
            },
            
            # ç‰©ç†å¼•æ“å’Œä»¿çœŸ
            {
                'name': 'Bullet Physics',
                'url': 'https://pybullet.org/wordpress/feed/',
                'category': 'physics_simulation'
            },
            {
                'name': 'NVIDIA PhysX',
                'url': 'https://developer.nvidia.com/rss.xml',
                'category': 'physics_simulation'
            },
            
            # åŠ¨ç”»æŠ€æœ¯
            {
                'name': 'Animation Mentor Blog',
                'url': 'https://www.animationmentor.com/blog/feed/',
                'category': 'animation'
            },
            {
                'name': 'Blender News',
                'url': 'https://www.blender.org/news/rss/',
                'category': 'animation'
            },
            
            # æ¸¸æˆå¼•æ“æ¶æ„
            {
                'name': 'Game Engine Architecture',
                'url': 'https://www.gameenginebook.com/feed/',
                'category': 'engine_architecture'
            },
            {
                'name': 'Molecular Musings',
                'url': 'https://blog.molecular-matters.com/feed/',
                'category': 'engine_architecture'
            },
            
            # AIå’Œæœºå™¨å­¦ä¹ åœ¨æ¸¸æˆä¸­çš„åº”ç”¨
            {
                'name': 'Unity ML-Agents',
                'url': 'https://blogs.unity3d.com/category/machine-learning/feed/',
                'category': 'ai_ml'
            },
            {
                'name': 'Game AI Pro',
                'url': 'http://www.gameaipro.com/feed/',
                'category': 'ai_ml'
            },
            
            # æ€§èƒ½ä¼˜åŒ–
            {
                'name': 'Intel Game Dev',
                'url': 'https://www.intel.com/content/www/us/en/developer/topic-technology/gamedev/rss.xml',
                'category': 'performance'
            },
            {
                'name': 'AMD GPUOpen',
                'url': 'https://gpuopen.com/feed/',
                'category': 'performance'
            },
            
            # VR/ARæŠ€æœ¯
            {
                'name': 'Oculus Developer Blog',
                'url': 'https://developer.oculus.com/blog/rss/',
                'category': 'vr_ar'
            },
            {
                'name': 'Unity XR',
                'url': 'https://blogs.unity3d.com/category/xr/feed/',
                'category': 'vr_ar'
            },
            
            # æŠ€æœ¯åšå®¢å’Œä¸ªäººåˆ†äº«
            {
                'name': 'Inigo Quilez',
                'url': 'https://iquilezles.org/articles/rss.xml',
                'category': 'technical_blogs'
            },
            {
                'name': 'Fabien Sanglard',
                'url': 'https://fabiensanglard.net/rss.xml',
                'category': 'technical_blogs'
            },
            {
                'name': 'Aras PranckeviÄius',
                'url': 'https://aras-p.info/blog/feed/',
                'category': 'technical_blogs'
            }
        ]
        
        for source_data in default_sources:
            existing = RSSSource.query.filter_by(url=source_data['url']).first()
            if not existing:
                source = RSSSource(**source_data)
                db.session.add(source)
        
        db.session.commit()

# å®šæ—¶ä»»åŠ¡
def start_scheduler():
    scheduler = BackgroundScheduler()
    scheduler.add_job(
        func=RSSFetcher.fetch_all_sources,
        trigger="interval",
        hours=2,  # æ¯2å°æ—¶æŠ“å–ä¸€æ¬¡
        id='fetch_rss'
    )
    scheduler.start()

if __name__ == '__main__':
    init_db()
    start_scheduler()
    app.run(debug=True, host='0.0.0.0', port=5000)